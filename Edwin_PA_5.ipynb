{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940d65bc-011d-4762-8f7c-d628f2a81bcd",
   "metadata": {},
   "source": [
    "# Programming Assignment #5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ccdfcc-83fa-4eb2-a3b7-9671f72157e8",
   "metadata": {},
   "source": [
    "## Association rule mining \n",
    "\n",
    "The market basket transactions dataset (transactions_data.txt)contains list of items purchased by customer in each transaction.\n",
    "\n",
    "- load the transaction dataset file\n",
    "- use minimum support = 0.2 and use_colname=True in apriori method \n",
    "- select metric as confidence in association rules\n",
    "- use minimum threshold = 0.5\n",
    "\n",
    "Ex: If the minimum support is 0.4, the metric is confidence and minimum threshold is 0.5 then some of the outputs are: \n",
    "- the least frequency of frequent 1-itemset is ['Queso'].\n",
    "- the support, confidence, and lift of rule, ['Queso'] -> ['Tortilla chips'] are:\n",
    "  - consequent support = 0.7\n",
    "  - support = 0.4\n",
    "  - confidence = 1.00\n",
    "  - lift = 1.42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eef59b37-5429-42b9-a45c-56ad45a7968c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "    support                        itemsets\n",
      "0       0.2                     (Guacamole)\n",
      "1       0.2                    (Pita chips)\n",
      "2       0.4                         (Queso)\n",
      "3       0.3                     (Ranch dip)\n",
      "4       0.6                         (Salsa)\n",
      "5       0.7                (Tortilla chips)\n",
      "6       0.2     (Guacamole, Tortilla chips)\n",
      "7       0.3                  (Queso, Salsa)\n",
      "8       0.4         (Queso, Tortilla chips)\n",
      "9       0.5         (Salsa, Tortilla chips)\n",
      "10      0.3  (Queso, Salsa, Tortilla chips)\n",
      "\n",
      "Association Rules:\n",
      "Itemset with the highest frequency for frequent 1-itemset: frozenset({'Tortilla chips'})\n",
      "Itemset with the least frequency for frequent 2-itemset: frozenset({'Guacamole', 'Tortilla chips'})\n",
      "Consequent support for ['Salsa', 'Tortilla chips']: 0.5\n",
      "Confidence for the rule ['Queso'] -> ['Salsa', 'Tortilla chips']: 0.7499999999999999\n",
      "Lift for the rule ['Queso'] -> ['Salsa', 'Tortilla chips']: 1.4999999999999998\n",
      "                antecedents              consequents  antecedent support  \\\n",
      "0               (Guacamole)         (Tortilla chips)                 0.2   \n",
      "1                   (Queso)                  (Salsa)                 0.4   \n",
      "2                   (Salsa)                  (Queso)                 0.6   \n",
      "3                   (Queso)         (Tortilla chips)                 0.4   \n",
      "4          (Tortilla chips)                  (Queso)                 0.7   \n",
      "5                   (Salsa)         (Tortilla chips)                 0.6   \n",
      "6          (Tortilla chips)                  (Salsa)                 0.7   \n",
      "7            (Queso, Salsa)         (Tortilla chips)                 0.3   \n",
      "8   (Queso, Tortilla chips)                  (Salsa)                 0.4   \n",
      "9   (Salsa, Tortilla chips)                  (Queso)                 0.5   \n",
      "10                  (Queso)  (Salsa, Tortilla chips)                 0.4   \n",
      "11                  (Salsa)  (Queso, Tortilla chips)                 0.6   \n",
      "\n",
      "    consequent support  support  confidence      lift  leverage  conviction  \\\n",
      "0                  0.7      0.2    1.000000  1.428571      0.06         inf   \n",
      "1                  0.6      0.3    0.750000  1.250000      0.06         1.6   \n",
      "2                  0.4      0.3    0.500000  1.250000      0.06         1.2   \n",
      "3                  0.7      0.4    1.000000  1.428571      0.12         inf   \n",
      "4                  0.4      0.4    0.571429  1.428571      0.12         1.4   \n",
      "5                  0.7      0.5    0.833333  1.190476      0.08         1.8   \n",
      "6                  0.6      0.5    0.714286  1.190476      0.08         1.4   \n",
      "7                  0.7      0.3    1.000000  1.428571      0.09         inf   \n",
      "8                  0.6      0.3    0.750000  1.250000      0.06         1.6   \n",
      "9                  0.4      0.3    0.600000  1.500000      0.10         1.5   \n",
      "10                 0.5      0.3    0.750000  1.500000      0.10         2.0   \n",
      "11                 0.4      0.3    0.500000  1.250000      0.06         1.2   \n",
      "\n",
      "    zhangs_metric  \n",
      "0        0.375000  \n",
      "1        0.333333  \n",
      "2        0.500000  \n",
      "3        0.500000  \n",
      "4        1.000000  \n",
      "5        0.400000  \n",
      "6        0.533333  \n",
      "7        0.428571  \n",
      "8        0.333333  \n",
      "9        0.666667  \n",
      "10       0.555556  \n",
      "11       0.500000  \n"
     ]
    }
   ],
   "source": [
    "# Import the packages\n",
    "# Import the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Loading the data\n",
    "def load_dataset(path_to_data):\n",
    "    transactions = []\n",
    "    with open(path_to_data, 'r') as fid:\n",
    "        for line in fid:\n",
    "            # Split each line into items and strip whitespace\n",
    "            transaction = line.strip().split(',')\n",
    "            transactions.append(transaction)\n",
    "    return transactions\n",
    "\n",
    "# Set the path to the data file\n",
    "path_to_data = \"transactions_data.txt\"  \n",
    "dataset = load_dataset(path_to_data)\n",
    "\n",
    "# Transform the data to a format suitable for the apriori function\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apply the apriori algorithm with a minimum support threshold, e.g., 0.2\n",
    "frequent_itemsets = apriori(df, min_support=0.2, use_colnames=True)\n",
    "print(\"Frequent Itemsets:\")\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Generate the association rules with a minimum confidence threshold, e.g., 0.5\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "print(\"\\nAssociation Rules:\")\n",
    "\n",
    "# Filter 1-itemsets and find the one with the highest support\n",
    "frequent_1_itemsets = frequent_itemsets[frequent_itemsets['itemsets'].apply(len) == 1]\n",
    "highest_freq_itemset = frequent_1_itemsets.loc[frequent_1_itemsets['support'].idxmax()]\n",
    "print(\"Itemset with the highest frequency for frequent 1-itemset:\", highest_freq_itemset['itemsets'])\n",
    "\n",
    "# Filter 2-itemsets and find the one with the lowest support\n",
    "frequent_2_itemsets = frequent_itemsets[frequent_itemsets['itemsets'].apply(len) == 2]\n",
    "least_freq_itemset = frequent_2_itemsets.loc[frequent_2_itemsets['support'].idxmin()]\n",
    "print(\"Itemset with the least frequency for frequent 2-itemset:\", least_freq_itemset['itemsets'])\n",
    "\n",
    "# Find the support for the consequent ['Salsa', 'Tortilla chips']\n",
    "consequent_support = frequent_itemsets[frequent_itemsets['itemsets'] == {'Salsa', 'Tortilla chips'}]['support']\n",
    "if not consequent_support.empty:\n",
    "    print(\"Consequent support for ['Salsa', 'Tortilla chips']:\", consequent_support.values[0])\n",
    "else:\n",
    "    print(\"Consequent ['Salsa', 'Tortilla chips'] not found in frequent itemsets.\")\n",
    "\n",
    "# Filter for the specific rule: ['Queso'] -> ['Salsa', 'Tortilla chips']\n",
    "specific_rule = rules[(rules['antecedents'] == {'Queso'}) & (rules['consequents'] == {'Salsa', 'Tortilla chips'})]\n",
    "\n",
    "# Check if the rule exists and print its confidence\n",
    "if not specific_rule.empty:\n",
    "    confidence_value = specific_rule['confidence'].values[0]\n",
    "    print(\"Confidence for the rule ['Queso'] -> ['Salsa', 'Tortilla chips']:\", confidence_value)\n",
    "else:\n",
    "    print(\"The rule ['Queso'] -> ['Salsa', 'Tortilla chips'] was not found with the current settings.\")\n",
    "\n",
    "# Filter for the specific rule: ['Queso'] -> ['Salsa', 'Tortilla chips']\n",
    "specific_rule = rules[(rules['antecedents'] == {'Queso'}) & (rules['consequents'] == {'Salsa', 'Tortilla chips'})]\n",
    "\n",
    "# Check if the rule exists and print its lift\n",
    "if not specific_rule.empty:\n",
    "    lift_value = specific_rule['lift'].values[0]\n",
    "    print(\"Lift for the rule ['Queso'] -> ['Salsa', 'Tortilla chips']:\", lift_value)\n",
    "else:\n",
    "    print(\"The rule ['Queso'] -> ['Salsa', 'Tortilla chips'] was not found with the current settings.\")\n",
    "\n",
    "print(rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81071e13-02cb-40f2-8210-5e420bf572ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
